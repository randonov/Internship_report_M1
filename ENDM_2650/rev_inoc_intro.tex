\section{Introduction}

Modern Next-Generation Sequencing  (NGS) techniques  are not able to output the whole genome sequence in one large sequence, but instead output billions of short DNA sequences, called \textit{reads}.  These reads  are extremely redundant, erroneous, and, consequently, unusable  practically. \textit{Genome assembly} is the challenging computational task, consisting in reconstructing the full genome sequence from these fragmented raw data. This  is a complex procedure, usually composed of three main steps: (1) generation of \textit{contigs}, which are long contiguous DNA sequences issued from the overlapping of the reads; (2) orientation and ordering of these contigs, called the \textit{scaffolding}; (3) gap-filling. A notable weakness of the contemporary approaches for \emph{de-novo assembly} is to consider the above process as a set of independent tasks and not be able to propose a global optimal solution.

The first step generates a list of \textit{contigs} (assembled ungapped sequences) that represent the "easily assembled regions"  of the genome.  Building contigs is currently supported by methods using a specific data structure called  \emph{de-Bruijn} graph \cite{Pevzner14082001}.
\forLaterVersions{, whose vertices  are all $k$-mers ($k$-length subwords of the reads) and whose edges connect all pairs of $k$-mers that share $k-1$ consecutive characters. Genomes are then sought as maximal unambiguous paths in the de Bruijn graph.
A lot of work has been done on this topic and the community profits today from  the development of efficient contig generators % and a very compact structure for representing de-Bruijn graphs with Bloom filters 
\cite{minia,others??}.  
However, complex regions of the genome (i.e. regions with many repeats) generally fail to be assembled by this technique: if there are repeats (identical subregions of the genome) longer than the size of the reads, the entire genome cannot be built in a unique way. }%Various heuristics are used to bypass simple repeats, but they do not guarantee correct solution
%

Whereas the main challenge in the first step is in the sheer size of the raw data,  in the second step, scaffolding, the data is of moderate size, but the problem remains largely open  because of its NP-hard complexity \cite{huson_greedy_2002}).  The goal here is to provide a reliable order and orientation of the contigs in order to link them together into \textit{scaffolds}-- a sequence of contigs that overlap or are separated by gaps of a given length. The gap information is generated during sequencing based on \textit{paired-end} or \textit{mate-pair} reads \cite{Weber01051997\forLaterVersions{,Medvedev_11}}, and has distance information associated with the gap. Specifically, each gap can be represented as a couple of fragments separated by a known distance (called \textit{insert size}) and provides information about the distance between the corresponding contigs. 
%They bring a long distance information that is missing in the first step and that can be %used for connecting contigs generated by de-Bruijn graphs.   

%Most previous work on scaffolding is heuristics based, e.g., SSPACE~\cite{boetzer_scaffolding_2011}, GRASS~\cite{Gritsenko01062012}, and BESST~\cite{BESST}. These scaffolders may find %in some cases  good solutions, but their accuracies cannot be guaranteed or predicted.  As far as we know, no method exists today that truly models the entire contigs relationship  and exactly solves  the underlying optimization problem.% in case of large and complex genomes \cite{Bosi25032015,ilp_montpellier}. 

\forLaterVersions{In addition to be NP-hard,  the scaffolding is challenging  because of technology  deficiencies like:
\begin{itemize}
	\item 
	Insert sizes are not precise. The technology provides approximate distance information only.
%	\item Distances between contigs may be larger than the insert size. In that case, scaffolding the whole genome is not possible. The solution will be a set of scaffolds;
%	\todo{this paragraph has been commented out. I am not sure how we recognize this case--we know the insert sizes, but not the distances. Then how do we decide whether to connect the scaffolds or keep them separated? In any case, we do not produce a set of scaffolds.}
	\item For short contigs, usually related to short repeat regions, multiplicity (the repeat factor) cannot be precisely determined. %This information is given by analyzing the coverage. 
	Shorter the contig, worse the estimation.
	\item There may be erroneous contigs. Heuristics implemented for generating contigs may lead to chimeric sequences that wrongly connect two regions of the genome.
\end{itemize}
}

While the ultimate goal of the genome assembly is to generate a complete genome, the scaffolding phase usually produces a set of multiple scaffolds that, in addition, may contain inside them regions that have not been completely predicted. %For example, for two contigs that have been unambiguously  linked,  the nucleotides sequence between them may have not been determined due to sequencing problem, or very high structure complexity. 
Further stages, such as  \textit{gap-filling} and  a step that we call here  \textit{scaffold extension} (elongating and concatenating the contigs after  the scaffolding step) are needed  to complete the genome. %enhance the scaffold. 

This paper focuses simultaneously on the above mentioned three  steps (scaffolding, gap filling and scaffold extension)  of de nouveau genome assembly.  Given a set of contigs and their relationships--overlaps and/or remoteness  in terms of distances between them (insert sizes)--we propose a global  optimization-based approach for completing  the genome assembly as the longest sequence that is consistent with the given contigs and linkage information. 
A drawback of the typically used strategy of constructing a set of disjoint paths, rather than a single path, is that it would require additional steps of gap filling and scaffold extension, involving additional work. Moreover, it would make impossible to find a provably optimal  final solution, since, even if each separate problem is implemented optimally, their combination may not be optimal.


Here we introduce a so called   \textit{contig graph}, that encodes information about contigs and distances between them,
%whose vertices are the contigs and whose edges connect pairs of contigs that either overlap, or have a gap of size given by the insert-size information. Edges have weights that encode the corresponding distance information between the contigs and are negative in the case of overlaps and positive in the case of gaps. Vertices have weights equal to the lengths of the contigs they represent. Contigs with repeat factor $s$ are represented as a set of $s$ vertices with the same sets of neighbors.  The length of a path in the resulting graph is defined as the sum of the weights of the vertices and edges in it.  
and reduce the  scaffolding and gap-filling  to finding a longest simple path in that graph such that as many as possible mate-pairs distances are satisfied  (we call hereafter such path just a \textit{longest path}).  Since both conditions cannot generally be simultaneously satisfied, our objective function is a linear combination of them.  We solve this problem by reformulating it as a mixed integer linear program (MILP) and develop a method that  exactly solves the resulting program on genomes of up to 165 contigs and up to 6682  binary variables. We analyze the performance of the algorithm on several chloroplast genomes and compare it to other scaffolding algorithms. 

An advantage of our approach is that the modeling of scaffolding as a longest path problem allows one to solve simultaneously all % several 
subtasks specific for completing the genome assembly. %  like: contig orientation and ordering, repeats,  gap filling, and scaffold extension, which in other approaches are targeted as separate problems. 
We are not aware of previous approaches on scaffolding based on the longest path problem reduction.  There is no guarantee that the genome sequence corresponds to a longest path, but our experiments show that that is the case in many instances or, if not, there is a very small difference between the two.  Unlike the shortest path problem with non-negative weights, for which efficient polynomial-time algorithms exist,
the longest weighted path problem is NP-hard \cite{Garey:1990:CIG:574848}, which means that no polynomial time solution is likely to exist for it. 

We tested this model on a set of chloroplast and bacteria  genome data  and showed that it allows  to assemble the complete genome as a single scaffold. Compared to the publicly available scaffolding tools that we have tested, our solution produces assemblies of significantly higher quality. 

Most previous work on scaffolding is heuristics based, e.g., SSPACE~\cite{boetzer_scaffolding_2011}, GRASS~\cite{Gritsenko01062012}, and BESST~\cite{BESST}. Such algorithms may find in some cases good solutions, but their accuracies cannot be guaranteed or predicted.  %In contrast, our method always finds a longest path in the contig graph. 
Exact algorithms for the scaffolding problem are presented in~\cite{weller2015exact}, but the focus of that work is on finding structural properties of the contig graph that will make the optimization problem of polynomial complexity.  In \cite{ilp_montpellier}, integer linear programming is used to model the scaffolding problem, with an objective  to maximize the number of links that are satisfied.  In order to avoid sub-cycles in the  solution, the authors use 
%do not use subcycle elimination constraint, but solve the problem 
 an incremental process, where cycles that may have been produced by the solver are forbidden in the next iteration.   %It is difficult to compare their approach with ours.  
 While our focus is on accuracy,  \cite{ilp_montpellier} focuses on efficiency, and indeed their algorithm, being a kind of heuristics, is faster than ours. 
However,  integrating the distances between contigs  and  accounting for possible multiplicities of the contigs (repeats) is indicated  as future improvement in 
 \cite{ilp_montpellier},  while it has been realized in our approach. 
 
%In contrast, our objective is to maximize the length of the resulting scaffold. Moreover, we aim at producing a single path or cycle, rather than a set of paths and cycles. %We believe that by requiring our solution to be a single path we avoid the risk of producing a set of paths for which the objective function is of  high value, but which are inconsistent with a single path ordering. 


\smallskip
The contributions  of this study are as follows:\vspace*{-0.2cm}
\begin{itemize}
\item   Our modeling of the scaffolding problem as a longest path problem allows to solve \emph{simultaneously} the set of subtasks specific for %this problem 
completing the genome assembly  like: contigs orientation and ordering, repeats,  gap filling and scaffold extension, which in other approaches are separate phases. 
\item  The  scaffolding problem  is reduced  to finding a longest path in a particular graph. In addition, these paths need to satisfy a set of distances between  couples of vertices along these paths. We are not aware of previous approaches on scaffolding based on the longest path problem. 
%The disadvantage of having a set of disjoint paths, rather than a single path, is that it would require additional steps of gap filling and scaffold extension, involving additional work. Moreover, it would make it impossible to produce an optimal  final solution, since, even if each separate problem is implemented optimally, their combination may not be optimal. 

\item   We formulate the above problem as a mixed integer linear program (MILP) with several interesting properties like:  cycles elimination constraints and using binary variables for the edges of the graph only.  Vertices are modeled with real variables, but we prove that the integrality of these variables follows from other constraints. Moreover,  the commonly used approach for solving the longest weighted simple path when the initial (source) and final (target) vertices are unknown consists in artificially adding these two vertices and $2|V|$ edges in the graph $G=(V,E)$ \cite{Bui2016}. This increases by $2|V|$ the number of binary variables, which is a drawback when the density of the graph is small (as is the case of scaffolding graph). In contrast, our modeling does not require such a graph transformation and requires fewer binary variables.

\item   We tested this model on a set of chloroplast and bacteria  genome data  and showed that it allows  to assemble the complete genome as a single scaffold. None of the publicly available scaffolding tools that we have tested targets  single scaffolds (this is corroborated by the obtained numerical results). 
\item Our numerical experiments indicate that the relaxation of the mixed integer model is tight and produces upper bounds of excellent quality. This suggests a promising direction of research towards the scalability of our approach.

\end{itemize}

\forLaterVersions{In the next Section 2 we describe our graph model and the formulation of the optimization problem and in Section 3 we present experimental results and comparison with other algorithms.}
